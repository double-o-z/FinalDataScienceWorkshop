{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7ceef31e34b944ae9630a3fafbf9d692": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5dd15acadc4a4c57a0fb87f60c703f53",
              "IPY_MODEL_870236f3321e43708d3ac47cc803b7fa",
              "IPY_MODEL_d527b21db0ee4e51a9fc23fdd2a9a2bb"
            ],
            "layout": "IPY_MODEL_638ec474e8c744bcbc2df060053f46bc"
          }
        },
        "5dd15acadc4a4c57a0fb87f60c703f53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f9a33e66a4e42a5acee38e3cdc0e9a3",
            "placeholder": "​",
            "style": "IPY_MODEL_4e0559db7a6d4a169b8abc9d3574b7e6",
            "value": "100%"
          }
        },
        "870236f3321e43708d3ac47cc803b7fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29b2ca10290c4cf4b37494507f4328b4",
            "max": 300,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eeed54b8210d4c488d430b01ebbf2853",
            "value": 300
          }
        },
        "d527b21db0ee4e51a9fc23fdd2a9a2bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a382ee968ca64380915ff16674c2dfb1",
            "placeholder": "​",
            "style": "IPY_MODEL_4e628a6637ab45e785cb7ce2b3c566c1",
            "value": " 300/300 [00:01&lt;00:00, 326.65it/s]"
          }
        },
        "638ec474e8c744bcbc2df060053f46bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f9a33e66a4e42a5acee38e3cdc0e9a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e0559db7a6d4a169b8abc9d3574b7e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "29b2ca10290c4cf4b37494507f4328b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eeed54b8210d4c488d430b01ebbf2853": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a382ee968ca64380915ff16674c2dfb1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e628a6637ab45e785cb7ce2b3c566c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Intro ###\n",
        "\n",
        "I'm Or Ohev-Zion, a Computer Science student at The Open University of Israel.\n",
        "The following document is a project developed for study purposes for a course: Data Science Workshop.\n",
        "\n",
        "As a veteran officer in the IDF, serving in a field technological unit, which uses various technological platforms and tools that enhance the IDF's combat abilities, I was intigured by the current technological development and issues of combat drones and wanted to learn how to solve an issue such technologies face today.\n",
        "\n",
        "A military drone which has the mission of performing object detection in the field, might come across several hardships in the process.\n",
        "Some of them are: poor or varying light conditions, obstructions in the field of view, varying sizes of objects (tiny to large), cluttering of images by rubble or backgroud objects like sand, water, concrete, and overlapping of objects.\n",
        "\n"
      ],
      "metadata": {
        "id": "lPe2VPyOYsOp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Description](clip-2-superJumbo.jpg)\n"
      ],
      "metadata": {
        "id": "oPwffkGNmxbO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When thinking on a subject for this course, I wanted to find a way to implement some machine learning model that can take the above mentioned issues and hardships and overcome them.\n",
        "This of course would have to be done methodically, as I was unable (and will not even if could), use real combat footage.\n",
        "To achieve this, I've decided to use animated footage from a video game (namely Baldurs Gate 3), which imitates this conditions.\n",
        "Then, I've design and implemented a series of steps to create model that is able to detect objects in a simulated combat environment.\n",
        "\n",
        "Here we can see an illustration of how this video game footage can simulate similar data by incorporating data which has varying light conditions, clutter and varying distances and sizes of objects of the same class. This is why this was used for data creation.\n"
      ],
      "metadata": {
        "id": "uz1uUv9Aiy5-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Description](frame_000009.png)\n"
      ],
      "metadata": {
        "id": "5a7IbfMAo-uk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first step was to record video data from the game (using a PS5, and uploading to the cloud - google drive).\n",
        "Next, I've used ffmpeg (as seen below) to separate the video clip into frames (or images) - one frame per second - for ease of use and dataset creation.\n",
        "\n",
        "The frames where uploaded to the same cloud into a \"frames\" folder, which will be the location of the initial raw data."
      ],
      "metadata": {
        "id": "E7I2m6LPjb1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# 1. Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "dataset_root = \"/content/drive/MyDrive/AI Workshop/YOLODataset/SingleRunDataset\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEKX9M-vvLyT",
        "outputId": "08c558fc-c220-4bb8-cdf6-61fb8d8d61c9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install ffmpeg\n",
        "!ffmpeg -version\n",
        "\n",
        "\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# 1. Define source and destination directories\n",
        "frames_root = os.path.join(dataset_root, \"frames\")  # Where extracted frames will be saved\n",
        "\n",
        "# 2. Find all .webm files under dataset_root (recursively)\n",
        "webm_files = glob.glob(dataset_root + '/**/*.webm', recursive=True)\n",
        "print(f\"Found {len(webm_files)} .webm files:\")\n",
        "\n",
        "for f in webm_files:\n",
        "    print(\"-\", f)\n",
        "\n",
        "# 3. For each .webm file, create a unique output folder, then run ffmpeg\n",
        "for webm_path in webm_files:\n",
        "    # Extract the base filename (without extension) for naming the subfolder\n",
        "    base_name = os.path.splitext(os.path.basename(webm_path))[0]\n",
        "\n",
        "    # e.g., if webm_path is \".../PS5/clip1.webm\", base_name = \"clip1\"\n",
        "    output_folder = os.path.join(frames_root, base_name)\n",
        "\n",
        "    # Create the output folder (if it doesn't exist)\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    # Build the ffmpeg command\n",
        "    # -i \"webm_path\": input file\n",
        "    # -vf fps=1: extract 1 frame per second\n",
        "    # output_folder/frame_%06d.jpg: store frames as frame_000001.jpg, frame_000002.jpg, etc.\n",
        "    ffmpeg_cmd = f'ffmpeg -i \"{webm_path}\" -vf fps=1 -pix_fmt rgb24 \"{output_folder}/frame_%06d.png\"'\n",
        "\n",
        "    print(f\"\\nExtracting frames for: {webm_path}\\nSaving to: {output_folder}\")\n",
        "\n",
        "    # 4. Run the ffmpeg command in a shell\n",
        "    !$ffmpeg_cmd\n",
        "\n",
        "print(\"\\nAll .webm files processed!\")"
      ],
      "metadata": {
        "id": "ECRdydCMbesZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data preparation and EDA ###\n",
        "\n",
        "An important step was to first decide on how to create data to train on.\n",
        "Initially, many different objects in the footage where categorized under the same class - which made the model mistake them, or be unable to really find them when predicting on new data.\n",
        "\n",
        "In various iterations of the process of creating the data, and doing some basic ML on it (specified how in next chapters), I was able to get better results.\n",
        "\n",
        "I've learned to define unique classes (of objects) to unique real objects in the footage. For example, because of the high variance in look between two different characters, even when sharing the same status or job (in the scene), it was better to classify them as different objects.\n",
        "\n",
        "This prepares the ground to a realization, that it is possibly better to train an object detection model to classify different objects of the same type, i.e. \"Enemy1\", \"Enemy2\", .. and \"Friendly1\", \"Friendly2\", .., instead of just \"Enemy\" and \"Friendly\" for all simialr objects of these categories.\n",
        "\n",
        "The next step was to manually label the initial frames dataset.\n",
        "This was done using an online tool: Roboflow.\n",
        "The tool allows to upload images and manually label shared classes to objects in the iamges, and later on creating a dataset from one of the common types (for this workshop purposes, PASCAL VOC was used, which uses the images, annotations file separation format)."
      ],
      "metadata": {
        "id": "As-TpaeXcEq2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Description](image_annotation_in_roboflow.png)\n"
      ],
      "metadata": {
        "id": "Hohlo5gqomxD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After creating a small but sufficient dataset (takes a long time of manual labor), it's possible to start working with the data and learning what ML models can make of it.\n",
        "\n",
        "### Model architecture choice ###\n",
        "In a separate course, namely AI Seminar, I've researched a hybrid cnn-trasnformers model, using various modern research papers published on arXiv.org, to desing a ML model that will be a good fit for this technology defined above (object detection for military drones in combat scense).\n",
        "\n",
        "I've decided to use YOLO for this purpose as it was the cnn part of the model architecture for various reason.\n",
        "\n",
        "For this reason, I've decided to focus on training a model using this architecture alone, using Keras code library for python.\n",
        "\n",
        "Many hyperparamters choices were considered to get a good result, and also different sub-models were used (from YOLOv8n to YOLOv8XL), to see which one performed best, and still getting close to real-time performace for the sake of usage in real combat conditions.\n",
        "\n",
        "### Training and prediction score ###\n",
        "\n",
        "The first few iterations got very bad results, due to learning mostly backgroud noise. Because it was hard to calculate metrics for varying amount of objects existing in each image, a good measure of accuracy in detection was to view how the model can predict by checking the results manually - by looking at the bounding boxes and their locations in the validation dataset. Because the dataset is rather small (about 100 images), it was easy to do.\n",
        "\n",
        "During training the best performing model was saves to disk (cloud), and reloaded once the training process ended, to start with predictions.\n",
        "\n"
      ],
      "metadata": {
        "id": "jqXVQ71xfMBQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although not shown in this notebook, the next step was to upload the images (frames) to Roboflow.com (free version), label 4 classes of objects in about 50 frames, download the dataset (images with metadata, specifying location of bounding boxes and labels on the frames) as Pascal VOC format (one xml file with metadata per frame (.jpg file of 640*640 resultion for comfort). Additionalty a few preprocessing techniques where performed on the dataset like \"resize\", \"auto-orient\". Then a few augmentation techniques to increase dataset size for more learning from a small dataset (\"flip\", \"90deg rotate\", \"rotation +-15deg\", \"shear +-10deg\") effectively making dataset of size 135 input images."
      ],
      "metadata": {
        "id": "dNOMSWVgVrIc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we set hyper-parameters for the model training, self-explanatory (pretty default values, like learning rate 0.001, split 80% train, 20% evaluation for dataset, 15 epochs - not too big, to avoid long training times, for easy reproduce by Teacher)."
      ],
      "metadata": {
        "id": "6H2-ShQIWwcm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters used throught the training"
      ],
      "metadata": {
        "id": "twrJESqItwud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SPLIT_RATIO = 0.2\n",
        "BATCH_SIZE = 4\n",
        "LEARNING_RATE = 0.001\n",
        "EPOCH = 15\n",
        "GLOBAL_CLIPNORM = 10.0"
      ],
      "metadata": {
        "id": "KDi7I-Y8Ch9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we install necessary packages for the model - specifically the project requirement is to use the Keras software pacakge which gives many tools for Deep Learning. Among those, the KerasCV module (which by documentation is in transition period into KerasHub, in the next year, but will remain usable) which supplies famous and state-of-the-art CNN pre-trained models. I've chosen to use the famous YOLO model as will be further explain ahead. Furthermore, I'm using Tensorflow for necessary methods to configure and train the model, like Optimizer, etc, which integrates well with Keras packages."
      ],
      "metadata": {
        "id": "moGCFWyhXGl3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade git+https://github.com/keras-team/keras-cv -q\n",
        "!pip install tensorflow --upgrade\n",
        "!pip install keras --upgrade\n",
        "!pip install keras-cv --upgrade\n"
      ],
      "metadata": {
        "id": "-3NmsaCj9Ae4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we import most of the necessary packages that will be used throughout the training of the model"
      ],
      "metadata": {
        "id": "E-MitPW2YNC0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from tqdm.auto import tqdm\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import keras_cv\n",
        "from keras_cv import bounding_box\n",
        "from keras_cv import visualization"
      ],
      "metadata": {
        "id": "aqH4H9u59Brc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, connect to drive, this time we fetch the images with metadata in correct format for training"
      ],
      "metadata": {
        "id": "XbRn_EdnYXKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths\n",
        "DATASET_ROOT = os.path.join(dataset_root, \"labeled_dataset_5\")"
      ],
      "metadata": {
        "id": "9-0Zxha99dLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we prepare the classes mapping, the images and annotations lists, to create the Dataset"
      ],
      "metadata": {
        "id": "JqWhw5c1YdNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_ids = [\n",
        "    \"Enemy Imp\",\n",
        "    \"Enemy Hellsboar\",\n",
        "    \"Player\",\n",
        "    \"Laezel\",\n",
        "    \"Shadowheart\",\n",
        "]\n",
        "\n",
        "class_mapping = dict(zip(range(len(class_ids)), class_ids))\n",
        "\n",
        "# Path to images and annotations\n",
        "path_images = os.path.join(DATASET_ROOT, \"images\")\n",
        "path_annot = os.path.join(DATASET_ROOT, \"annotations\")\n",
        "\n",
        "# Get all XML file paths in path_annot and sort them\n",
        "xml_files = sorted(\n",
        "    [\n",
        "        os.path.join(path_annot, file_name)\n",
        "        for file_name in os.listdir(path_annot)\n",
        "        if file_name.endswith(\".xml\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Get all JPEG image file paths in path_images and sort them\n",
        "jpg_files = sorted(\n",
        "    [\n",
        "        os.path.join(path_images, file_name)\n",
        "        for file_name in os.listdir(path_images)\n",
        "        if file_name.endswith(\".jpg\")\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "zR-sKUuX9Gw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jpg_files.__len__()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8Ew8OJHypKb",
        "outputId": "c1dcd693-abf9-4467-c662-a091fd5719b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we parse the annotations data, and bundle the images with them in the expected format, before initializing the dataset used by the model."
      ],
      "metadata": {
        "id": "T0IKVQ10YoHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_annotation(xml_file):\n",
        "    tree = ET.parse(xml_file)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    image_name = root.find(\"filename\").text\n",
        "    image_path = os.path.join(path_images, image_name)\n",
        "\n",
        "    boxes = []\n",
        "    classes = []\n",
        "    for obj in root.iter(\"object\"):\n",
        "        cls = obj.find(\"name\").text\n",
        "        classes.append(cls)\n",
        "\n",
        "        bbox = obj.find(\"bndbox\")\n",
        "        xmin = float(bbox.find(\"xmin\").text)\n",
        "        ymin = float(bbox.find(\"ymin\").text)\n",
        "        xmax = float(bbox.find(\"xmax\").text)\n",
        "        ymax = float(bbox.find(\"ymax\").text)\n",
        "        boxes.append([xmin, ymin, xmax, ymax])\n",
        "\n",
        "    class_ids = [\n",
        "        list(class_mapping.keys())[list(class_mapping.values()).index(cls)]\n",
        "        for cls in classes\n",
        "    ]\n",
        "    return image_path, boxes, class_ids\n",
        "\n",
        "\n",
        "image_paths = []\n",
        "bbox = []\n",
        "classes = []\n",
        "for xml_file in tqdm(xml_files):\n",
        "    image_path, boxes, class_ids = parse_annotation(xml_file)\n",
        "    image_paths.append(image_path)\n",
        "    bbox.append(boxes)\n",
        "    classes.append(class_ids)\n",
        "\n",
        "boxes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "7ceef31e34b944ae9630a3fafbf9d692",
            "5dd15acadc4a4c57a0fb87f60c703f53",
            "870236f3321e43708d3ac47cc803b7fa",
            "d527b21db0ee4e51a9fc23fdd2a9a2bb",
            "638ec474e8c744bcbc2df060053f46bc",
            "6f9a33e66a4e42a5acee38e3cdc0e9a3",
            "4e0559db7a6d4a169b8abc9d3574b7e6",
            "29b2ca10290c4cf4b37494507f4328b4",
            "eeed54b8210d4c488d430b01ebbf2853",
            "a382ee968ca64380915ff16674c2dfb1",
            "4e628a6637ab45e785cb7ce2b3c566c1"
          ]
        },
        "id": "nNZIYaRQCM2D",
        "outputId": "5524da7e-ee8a-44aa-fbf7-ae2af821108b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/300 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7ceef31e34b944ae9630a3fafbf9d692"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[238.0, 227.0, 416.0, 431.0]]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a necessary and important step for our mission. Because not all images have equal amount of bounding boxes (circling objects), we need to convert our dataset to a \"Ragged Tensor\" form, which allows the dataset to handle varying dimensions of objects. For example: Image 1, might have 10 detected object in it (or true labeled for that matter), and Image 2 might have only 3 detected objects. The dataset created here, is aware of the varying sizes and amounts of objects (both from true \"ground-truth\" object from labeled data, and from predicted objects (in evaluation data for example))."
      ],
      "metadata": {
        "id": "k56F836XY0d_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bbox = tf.ragged.constant(bbox)\n",
        "classes = tf.ragged.constant(classes)\n",
        "image_paths = tf.ragged.constant(image_paths)\n",
        "\n",
        "data = tf.data.Dataset.from_tensor_slices((image_paths, classes, bbox))"
      ],
      "metadata": {
        "id": "t38xfAkuCdKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we split the dataset into training and validation by the SPLIT_RATIO."
      ],
      "metadata": {
        "id": "wiU9OQN5Zgpi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine the number of validation samples\n",
        "num_val = int(len(xml_files) * SPLIT_RATIO)\n",
        "\n",
        "# Split the dataset into train and validation sets\n",
        "val_data = data.take(num_val)\n",
        "train_data = data.skip(num_val)"
      ],
      "metadata": {
        "id": "4xKhTDITCd1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consequently, the amount of files (images) for validation:"
      ],
      "metadata": {
        "id": "GTecCnuGZoV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_val"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJczJsad2ElS",
        "outputId": "e77f7ea5-06de-4ac7-d236-79ec489db253"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper methods to load project the data and batch it together - preparing for training."
      ],
      "metadata": {
        "id": "MTi0i9mNZvJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image(image_path):\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    return image\n",
        "\n",
        "\n",
        "def load_dataset(image_path, classes, bbox):\n",
        "    # Read Image\n",
        "    image = load_image(image_path)\n",
        "    bounding_boxes = {\n",
        "        \"classes\": tf.cast(classes, dtype=tf.float32),\n",
        "        \"boxes\": bbox,\n",
        "    }\n",
        "    return {\"images\": tf.cast(image, tf.float32), \"bounding_boxes\": bounding_boxes}"
      ],
      "metadata": {
        "id": "XFDaakpHC6no"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Augmentation ###\n",
        "\n",
        "An Augmentor layers bundle - which allows the model to first \"puff\" our data up by performing data science known methods to increase the input size - this is to artificially increase the amount of data the model can learn from - altough it is somewhat less effective then introducing new \"real\" data. I struggled getting results from the model, and decided to keep it simple, so i removed the fancy augmentations to make it easier for the model to classify objects. In hindsight, I realized that using binary_crossentropy was a bad choice, but before I did I removed the fancy augmentations. I got good results without Shear and Flip, so I kept only the resize."
      ],
      "metadata": {
        "id": "dGXogJcpZ9bA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "augmenter = keras.Sequential(\n",
        "    layers=[\n",
        "        keras_cv.layers.RandomFlip(mode=\"horizontal\", bounding_box_format=\"xyxy\"),\n",
        "        keras_cv.layers.RandomShear(\n",
        "            x_factor=0.2, y_factor=0.2, bounding_box_format=\"xyxy\"\n",
        "        ),\n",
        "        keras_cv.layers.JitteredResize(\n",
        "            target_size=(640, 640), scale_factor=(0.75, 1.3), bounding_box_format=\"xyxy\"\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "augmenter = keras.Sequential(\n",
        "    layers=[\n",
        "        keras_cv.layers.JitteredResize(\n",
        "            target_size=(640, 640), scale_factor=(1, 1), bounding_box_format=\"xyxy\"\n",
        "        ),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "by80rHJ9DG_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we prepare the \"training dataset\" for the model by setting a few operation, like batching the data together, for smarter and faster learning."
      ],
      "metadata": {
        "id": "v_Ni_spGaVPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = train_data.map(load_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "train_ds = train_ds.shuffle(BATCH_SIZE * 4)\n",
        "train_ds = train_ds.ragged_batch(BATCH_SIZE, drop_remainder=True)\n",
        "train_ds = train_ds.map(augmenter, num_parallel_calls=tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "ic8r6cfkDLWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we prepare the validation dataset, by performing a simple resize on the data for some of the data, just to let the model sweat a bit when predicting this new data (did not train on this data). Removed the scale factor, kept it simple."
      ],
      "metadata": {
        "id": "afTD4Y6jahzK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resizing = keras_cv.layers.JitteredResize(\n",
        "    target_size=(640, 640),\n",
        "    scale_factor=(1, 1),\n",
        "    bounding_box_format=\"xyxy\",\n",
        ")\n",
        "\n",
        "val_ds = val_data.map(load_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.shuffle(BATCH_SIZE * 4)\n",
        "val_ds = val_ds.ragged_batch(BATCH_SIZE, drop_remainder=True)\n",
        "val_ds = val_ds.map(resizing, num_parallel_calls=tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "4uLLK-nnDY0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we take a sneak peak on the images, with the labeled objects, before starting training, just to get a feel of how the images look, and how much data can be seen by the model and expected by it to predict itself. Reminder: this dataset was manually created by me for the purposes of trying to train a model to detect objects in difficult conditions (similar colors with background, varying distances from viewer, unfamiliar classes (probably the model never saw such objects during pre-training))."
      ],
      "metadata": {
        "id": "6retmuioavy8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_dataset(dataset, value_range, rows, cols, bounding_box_format):\n",
        "    \"\"\"\n",
        "    Iterates over all batches in the dataset and plots bounding boxes\n",
        "    using KerasCV's visualization utilities.\n",
        "\n",
        "    Parameters:\n",
        "      - dataset: a tf.data.Dataset whose elements are dictionaries\n",
        "                 with keys \"images\" and \"bounding_boxes\"\n",
        "      - value_range: tuple, e.g. (0, 255) for pixel range\n",
        "      - rows, cols: how to arrange the images in the gallery\n",
        "      - bounding_box_format: e.g. \"xyxy\"\n",
        "    \"\"\"\n",
        "    for batch in dataset:\n",
        "        # Expect batch to be a dictionary with \"images\" and \"bounding_boxes\"\n",
        "        images = batch[\"images\"]\n",
        "        bounding_boxes = batch[\"bounding_boxes\"]\n",
        "\n",
        "        visualization.plot_bounding_box_gallery(\n",
        "            images,\n",
        "            value_range=value_range,\n",
        "            rows=rows,\n",
        "            cols=cols,\n",
        "            y_true=bounding_boxes,\n",
        "            scale=5,\n",
        "            font_scale=0.7,\n",
        "            bounding_box_format=bounding_box_format,\n",
        "            class_mapping=class_mapping,\n",
        "        )\n",
        "\n",
        "\n",
        "visualize_dataset(\n",
        "    train_ds, bounding_box_format=\"xyxy\", value_range=(0, 255), rows=2, cols=2\n",
        ")\n",
        "\n",
        "visualize_dataset(\n",
        "    val_ds, bounding_box_format=\"xyxy\", value_range=(0, 255), rows=2, cols=2\n",
        ")"
      ],
      "metadata": {
        "id": "H8nKKqSTDcnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we prepare the data the last time, according to convention of the CNN model architecture"
      ],
      "metadata": {
        "id": "udaWE5tbbm0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dict_to_tuple(inputs):\n",
        "    return inputs[\"images\"], inputs[\"bounding_boxes\"]\n",
        "\n",
        "\n",
        "train_ds = train_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "val_ds = val_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "gmd_KqTFDfzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just checking the classes and boxes are populted correctly here."
      ],
      "metadata": {
        "id": "gv3u3GJifkUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for images, labels in train_ds.take(1):\n",
        "    print(labels)  # Check if the bounding boxes and classes are correctly structured\n",
        "    print(labels[\"classes\"])\n",
        "    print(labels[\"boxes\"])\n",
        "    print(images)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYXtZijxv0GL",
        "outputId": "3d0e1664-3e28-4282-a849-923500870f8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'classes': <tf.RaggedTensor [[0.0, 4.0, 1.0, 0.0], [2.0, 0.0, 1.0, 3.0, 0.0], [0.0, 4.0, 1.0, 0.0],\n",
            " [2.0, 4.0, 0.0]]>, 'boxes': <tf.RaggedTensor [[[300.0, 228.0, 334.0, 276.0],\n",
            "  [239.0, 505.0, 273.0, 579.0],\n",
            "  [421.0, 347.0, 476.0, 415.0],\n",
            "  [627.0, 262.0, 640.0, 305.0]], [[288.0, 510.0, 350.0, 622.0],\n",
            "                                  [476.0, 397.0, 527.0, 470.0],\n",
            "                                  [270.0, 313.0, 344.0, 402.0],\n",
            "                                  [246.0, 385.0, 276.0, 463.0],\n",
            "                                  [317.0, 176.0, 356.0, 231.0]],\n",
            " [[293.0, 231.0, 322.0, 275.0],\n",
            "  [275.0, 513.0, 299.0, 585.0],\n",
            "  [432.0, 332.0, 478.0, 394.0],\n",
            "  [622.0, 219.0, 640.0, 260.0]], [[306.0, 302.0, 346.0, 393.0],\n",
            "                                  [215.0, 392.0, 245.0, 465.0],\n",
            "                                  [187.0, 228.0, 221.0, 286.0]]]>}\n",
            "<tf.RaggedTensor [[0.0, 4.0, 1.0, 0.0], [2.0, 0.0, 1.0, 3.0, 0.0], [0.0, 4.0, 1.0, 0.0],\n",
            " [2.0, 4.0, 0.0]]>\n",
            "<tf.RaggedTensor [[[300.0, 228.0, 334.0, 276.0],\n",
            "  [239.0, 505.0, 273.0, 579.0],\n",
            "  [421.0, 347.0, 476.0, 415.0],\n",
            "  [627.0, 262.0, 640.0, 305.0]], [[288.0, 510.0, 350.0, 622.0],\n",
            "                                  [476.0, 397.0, 527.0, 470.0],\n",
            "                                  [270.0, 313.0, 344.0, 402.0],\n",
            "                                  [246.0, 385.0, 276.0, 463.0],\n",
            "                                  [317.0, 176.0, 356.0, 231.0]],\n",
            " [[293.0, 231.0, 322.0, 275.0],\n",
            "  [275.0, 513.0, 299.0, 585.0],\n",
            "  [432.0, 332.0, 478.0, 394.0],\n",
            "  [622.0, 219.0, 640.0, 260.0]], [[306.0, 302.0, 346.0, 393.0],\n",
            "                                  [215.0, 392.0, 245.0, 465.0],\n",
            "                                  [187.0, 228.0, 221.0, 286.0]]]>\n",
            "tf.Tensor(\n",
            "[[[[ 0.  0.  0.]\n",
            "   [ 0.  0.  0.]\n",
            "   [ 0.  0.  0.]\n",
            "   ...\n",
            "   [ 0.  0.  0.]\n",
            "   [ 0.  0.  0.]\n",
            "   [ 0.  0.  0.]]\n",
            "\n",
            "  [[ 0.  0.  0.]\n",
            "   [ 0.  0.  0.]\n",
            "   [ 0.  0.  0.]\n",
            "   ...\n",
            "   [ 0.  0.  0.]\n",
            "   [ 0.  0.  0.]\n",
            "   [ 0.  0.  0.]]\n",
            "\n",
            "  [[ 0.  0.  0.]\n",
            "   [ 0.  0.  0.]\n",
            "   [ 0.  0.  0.]\n",
            "   ...\n",
            "   [ 0.  0.  0.]\n",
            "   [ 0.  0.  0.]\n",
            "   [ 0.  0.  0.]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[ 0.  0.  0.]\n",
            "   [ 0.  0.  0.]\n",
            "   [ 0.  0.  0.]\n",
            "   ...\n",
            "   [ 0.  0.  0.]\n",
            "   [ 0.  0.  0.]\n",
            "   [ 0.  0.  0.]]\n",
            "\n",
            "  [[ 0.  0.  0.]\n",
            "   [ 0.  0.  0.]\n",
            "   [ 0.  0.  0.]\n",
            "   ...\n",
            "   [ 0.  0.  0.]\n",
            "   [ 0.  0.  0.]\n",
            "   [ 0.  0.  0.]]\n",
            "\n",
            "  [[ 0.  0.  0.]\n",
            "   [ 0.  0.  0.]\n",
            "   [ 0.  0.  0.]\n",
            "   ...\n",
            "   [ 0.  0.  0.]\n",
            "   [ 0.  0.  0.]\n",
            "   [ 0.  0.  0.]]]\n",
            "\n",
            "\n",
            " [[[ 0.  0.  0.]\n",
            "   [ 0.  0.  0.]\n",
            "   [ 0.  0.  0.]\n",
            "   ...\n",
            "   [ 0.  0.  0.]\n",
            "   [ 0.  0.  0.]\n",
            "   [ 0.  0.  0.]]\n",
            "\n",
            "  [[ 0.  0.  0.]\n",
            "   [ 0.  0.  0.]\n",
            "   [ 0.  0.  0.]\n",
            "   ...\n",
            "   [ 0.  0.  0.]\n",
            "   [ 0.  0.  0.]\n",
            "   [ 0.  0.  0.]]\n",
            "\n",
            "  [[ 0.  0.  0.]\n",
            "   [ 0.  0.  0.]\n",
            "   [ 0.  0.  0.]\n",
            "   ...\n",
            "   [ 0.  0.  0.]\n",
            "   [ 0.  0.  0.]\n",
            "   [ 0.  0.  0.]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[ 0.  0.  0.]\n",
            "   [ 0.  0.  0.]\n",
            "   [ 0.  0.  0.]\n",
            "   ...\n",
            "   [ 0.  0.  0.]\n",
            "   [ 0.  0.  0.]\n",
            "   [ 0.  0.  0.]]\n",
            "\n",
            "  [[ 0.  0.  0.]\n",
            "   [ 0.  0.  0.]\n",
            "   [ 0.  0.  0.]\n",
            "   ...\n",
            "   [ 0.  0.  0.]\n",
            "   [ 0.  0.  0.]\n",
            "   [ 0.  0.  0.]]\n",
            "\n",
            "  [[ 0.  0.  0.]\n",
            "   [ 0.  0.  0.]\n",
            "   [ 0.  0.  0.]\n",
            "   ...\n",
            "   [ 0.  0.  0.]\n",
            "   [ 0.  0.  0.]\n",
            "   [ 0.  0.  0.]]]\n",
            "\n",
            "\n",
            " [[[44. 42. 53.]\n",
            "   [44. 42. 53.]\n",
            "   [45. 43. 54.]\n",
            "   ...\n",
            "   [77. 71. 73.]\n",
            "   [74. 68. 70.]\n",
            "   [74. 68. 70.]]\n",
            "\n",
            "  [[44. 42. 53.]\n",
            "   [44. 42. 53.]\n",
            "   [45. 43. 54.]\n",
            "   ...\n",
            "   [77. 71. 73.]\n",
            "   [74. 68. 70.]\n",
            "   [72. 66. 68.]]\n",
            "\n",
            "  [[43. 41. 52.]\n",
            "   [44. 42. 53.]\n",
            "   [45. 43. 54.]\n",
            "   ...\n",
            "   [77. 71. 73.]\n",
            "   [73. 67. 69.]\n",
            "   [70. 64. 66.]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[29. 26. 35.]\n",
            "   [29. 26. 35.]\n",
            "   [29. 26. 35.]\n",
            "   ...\n",
            "   [86. 60. 59.]\n",
            "   [88. 62. 61.]\n",
            "   [92. 66. 65.]]\n",
            "\n",
            "  [[29. 26. 35.]\n",
            "   [29. 26. 35.]\n",
            "   [29. 26. 35.]\n",
            "   ...\n",
            "   [85. 59. 58.]\n",
            "   [86. 60. 59.]\n",
            "   [91. 65. 64.]]\n",
            "\n",
            "  [[29. 26. 35.]\n",
            "   [29. 26. 35.]\n",
            "   [29. 26. 35.]\n",
            "   ...\n",
            "   [84. 58. 57.]\n",
            "   [86. 60. 59.]\n",
            "   [91. 65. 64.]]]\n",
            "\n",
            "\n",
            " [[[72. 70. 75.]\n",
            "   [72. 70. 75.]\n",
            "   [71. 69. 74.]\n",
            "   ...\n",
            "   [78. 76. 77.]\n",
            "   [79. 77. 78.]\n",
            "   [79. 77. 78.]]\n",
            "\n",
            "  [[71. 69. 74.]\n",
            "   [71. 69. 74.]\n",
            "   [71. 69. 74.]\n",
            "   ...\n",
            "   [77. 75. 76.]\n",
            "   [80. 78. 79.]\n",
            "   [81. 79. 80.]]\n",
            "\n",
            "  [[70. 68. 73.]\n",
            "   [70. 68. 73.]\n",
            "   [70. 68. 73.]\n",
            "   ...\n",
            "   [77. 75. 76.]\n",
            "   [80. 78. 79.]\n",
            "   [83. 81. 82.]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[45. 43. 48.]\n",
            "   [46. 44. 49.]\n",
            "   [46. 44. 49.]\n",
            "   ...\n",
            "   [64. 59. 56.]\n",
            "   [67. 62. 59.]\n",
            "   [66. 61. 58.]]\n",
            "\n",
            "  [[47. 45. 50.]\n",
            "   [47. 45. 50.]\n",
            "   [47. 45. 50.]\n",
            "   ...\n",
            "   [64. 59. 56.]\n",
            "   [67. 62. 59.]\n",
            "   [66. 61. 58.]]\n",
            "\n",
            "  [[50. 48. 53.]\n",
            "   [50. 48. 53.]\n",
            "   [50. 48. 53.]\n",
            "   ...\n",
            "   [64. 59. 56.]\n",
            "   [67. 62. 59.]\n",
            "   [66. 61. 58.]]]], shape=(4, 640, 640, 3), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model setup and training ###\n",
        "\n",
        "This is an important part - here we specify the exact type of pre-trained CNN model used, namely \"YOLOv8 Large with COCO backbone\". This means it uses the YOLO CNN architecture of the 8th edition, with rather large and costly training and prediction costs (Small wasn't able to learn and predict this kind of data so I went for Large), and it was pre-trained on a very famous dataset of pre-labeled images called COCO dataset.\n",
        "\n",
        "We also configure to use the helper class by KerasCV - YOLOV8Detector, which is the code that runs the above model.\n",
        "\n",
        "Then we configure an Optimizer for the model, also fetched from Keras, with pretty default hyper-parameters values.\n",
        "\n",
        "Finally we compile everything together, while specifying pretty standard functions for calculating improvement in the training process \"binary crossentropy\" for the classes and \"Complete IoU\" for the bounding boxes (which takes into account both two-dimensional locations and sizes and shapes of the bbs).\n",
        "\n",
        "There is a lot to be said about the choices here - fpn_depth allows the model to handle varying sizes of objects, but because most of the objects I labeled are pretty similar in size, and after getting bad results (no detections) when playing (increasing) the fpn_depth, I kept it at 1.\n",
        "\n",
        "the score_threshold and nms_iou_threshold are inference parameters and not trianing parameters. After the model predicts on an image, it uses these parameters to decide how much of the predicted boxes to filter out, to avoid duplicated boxes and uncertain (low-confidence) detections. We can change these even after finishing training, and will play with them later.\n",
        "\n",
        "The bb_format is mandatory - xyxy - to fit the dataset structure (pascal voc format with x/y_min/max)."
      ],
      "metadata": {
        "id": "fq7uhlByb2Ja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial model setup\n",
        "backbone = keras_cv.models.YOLOV8Backbone.from_preset(\"yolo_v8_xl_backbone_coco\")\n",
        "\n",
        "yolo = keras_cv.models.YOLOV8Detector(\n",
        "    num_classes=len(class_mapping),\n",
        "    bounding_box_format=\"xyxy\",\n",
        "    backbone=backbone,\n",
        "    fpn_depth=1,\n",
        "    score_threshold=0.5,\n",
        "    nms_iou_threshold=0.5,\n",
        ")"
      ],
      "metadata": {
        "id": "Ejx4Xr_Z8JKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The callback \"checkpoint\" function, allows the model to monitor progress and keep the best fitting model between epochs. We keep its weight in a file, specified in the filepath param."
      ],
      "metadata": {
        "id": "drp7_VKNg11c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compilation methods\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=\"best_model2.keras\",\n",
        "    monitor=\"val_box_loss\",  # or another metric, e.g., \"val_box_loss\"\n",
        "    mode=\"min\",          # lower loss is better\n",
        "    save_best_only=True,\n",
        "    save_weights_only=False,\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_loss\",   # Watch validation loss\n",
        "    patience=2,           # Stop if no improvement for 3 epochs\n",
        "    min_delta=0.1,        # Minimum improvement to continue training\n",
        "    mode=\"min\",           # Lower `val_loss` is better\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "Za_OGqg742Us"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Every time before fitting, we need to compile the model with optimizer, classificatio loss function and box_loss function. I can't stress enough how important these are. For my case after many sweat and tears, I found the Focal Loss is mandatory for my dataset because it makes sure the model doesn't train on noise - resulting in zero predictions on actual objects we wish to find.\n",
        "\n",
        "As for the learning rate its okay to start high (0.001) and in future epochs can reduce it by an order of magnitude to make learning even mroe precise, after getting good grasp of basic understaning of the images and objects relevant. the global_clipnorm just makes sure we avoid overfitting (i think)."
      ],
      "metadata": {
        "id": "3V1HgFHXhEhF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup optimizer and compile\n",
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate=0.001,\n",
        "    global_clipnorm=GLOBAL_CLIPNORM,\n",
        ")\n",
        "\n",
        "yolo.compile(\n",
        "    optimizer=optimizer,\n",
        "    classification_loss=keras_cv.losses.FocalLoss(from_logits=True, gamma=2.0),  # 🟢 Use Focal Loss for best performance with different classes\n",
        "    box_loss=\"ciou\"\n",
        ")"
      ],
      "metadata": {
        "id": "qKO2FowJ48zv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we just check class balance - its quite okay. Not great bad not bad as well. So probably doesn't require using class weights."
      ],
      "metadata": {
        "id": "wFdT1BOmh9jO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "class_counts = Counter()\n",
        "for images, labels in train_ds:\n",
        "    # Access classes from the 'labels' dictionary\n",
        "    for cls_array in labels[\"classes\"]: # Iterate over sub-arrays\n",
        "        for cls in cls_array.numpy().tolist(): # Convert to list for iteration\n",
        "            # Convert cls to a hashable type, if needed\n",
        "            cls = int(cls)  # Assuming cls is now a single numeric value\n",
        "            class_counts[cls] += 1\n",
        "\n",
        "print(class_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDJZ6DwmTbMp",
        "outputId": "404c6938-fc1c-4fa7-e1b7-64f057935289"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({0: 255, 2: 168, 1: 159, 3: 135, 4: 81})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train CNN Model\n",
        "\n",
        "Here we finally start training the model. it can take about an hour to fine-tune the model with our dataset.\n",
        "\n",
        "After every epoch, we check and score our current model by testing its prediction abilities on the validation data - this way, it doesn't learn from the validation data, but uses it to increase the model's prediction capabilites.\n",
        "\n",
        "Here starts a game of training - checking predictions - re-training - re-checking, until getting satisfactory results. When I say training I mean fine-tuning the model - namely YOLOv8 which is great for this purpose. It specializes on object detection, and is modern and extremly fast relatively."
      ],
      "metadata": {
        "id": "omCdk4V2GC6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup training parameters and start training\n",
        "\n",
        "yolo.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=5,\n",
        "    callbacks=[checkpoint_callback],\n",
        ")"
      ],
      "metadata": {
        "id": "Il0iUSOFEkZN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "467bc8bc-fcb2-4bd3-b158-7ae65d508c22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py:237: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
            "Expected: ['keras_tensor_10597']\n",
            "Received: inputs=Tensor(shape=(4, 640, 640, 3))\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11s/step - box_loss: 3.8623 - class_loss: 0.0189 - loss: 3.8811 \n",
            "Epoch 1: val_box_loss improved from inf to 4.84277, saving model to best_model2.keras\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m759s\u001b[0m 12s/step - box_loss: 3.8526 - class_loss: 0.0188 - loss: 3.8714 - val_box_loss: 4.8428 - val_class_loss: 0.0417 - val_loss: 4.8845\n",
            "Epoch 2/5\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11s/step - box_loss: 2.5884 - class_loss: 0.0076 - loss: 2.5961 \n",
            "Epoch 2: val_box_loss improved from 4.84277 to 3.52260, saving model to best_model2.keras\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m700s\u001b[0m 12s/step - box_loss: 2.5872 - class_loss: 0.0077 - loss: 2.5948 - val_box_loss: 3.5226 - val_class_loss: 0.0086 - val_loss: 3.5312\n",
            "Epoch 3/5\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11s/step - box_loss: 2.4026 - class_loss: 0.0068 - loss: 2.4094 \n",
            "Epoch 3: val_box_loss improved from 3.52260 to 2.82485, saving model to best_model2.keras\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m705s\u001b[0m 12s/step - box_loss: 2.4021 - class_loss: 0.0068 - loss: 2.4089 - val_box_loss: 2.8248 - val_class_loss: 0.0059 - val_loss: 2.8307\n",
            "Epoch 4/5\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11s/step - box_loss: 2.2836 - class_loss: 0.0066 - loss: 2.2902 \n",
            "Epoch 4: val_box_loss did not improve from 2.82485\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m720s\u001b[0m 12s/step - box_loss: 2.2829 - class_loss: 0.0066 - loss: 2.2895 - val_box_loss: 4.3344 - val_class_loss: 0.0086 - val_loss: 4.3430\n",
            "Epoch 5/5\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11s/step - box_loss: 2.2228 - class_loss: 0.0059 - loss: 2.2287 \n",
            "Epoch 5: val_box_loss did not improve from 2.82485\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m720s\u001b[0m 12s/step - box_loss: 2.2220 - class_loss: 0.0059 - loss: 2.2279 - val_box_loss: 3.7176 - val_class_loss: 0.0120 - val_loss: 3.7296\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7e3f3ca30890>"
            ]
          },
          "metadata": {},
          "execution_count": 244
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By now, we already have a trained model, and we can get the optimal one according to prediction score from memory.\n",
        "\n",
        "Here we can load the best trained model's weights from file system if needed. We can probably save it on drive, to avoid losing it between runs, but for this project its okay like this."
      ],
      "metadata": {
        "id": "zXlxPJL5ei24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yolo = keras_cv.models.YOLOV8Detector(\n",
        "    num_classes=len(class_mapping),\n",
        "    bounding_box_format=\"xyxy\",\n",
        "    backbone=keras_cv.models.YOLOV8Backbone.from_preset(\"yolo_v8_xl_backbone_coco\"),\n",
        ")\n",
        "\n",
        "yolo.load_weights(\"best_model3.keras\")"
      ],
      "metadata": {
        "id": "J2gpvP1HeFWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used this to quickly load the model, although it caused some issues, once I start playing with fpn_depth between epochs. Best to just load the weights instead. But this is also one way to load the already fine-tuned model."
      ],
      "metadata": {
        "id": "gsJFXd-Wi208"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import custom_object_scope\n",
        "import keras_cv\n",
        "\n",
        "# Include all necessary YOLOv8 classes and import BoundingBoxes from the correct location\n",
        "custom_objects = {\n",
        "    \"YOLOV8Detector\": keras_cv.models.YOLOV8Detector,\n",
        "    \"YOLOV8Backbone\": keras_cv.models.YOLOV8Backbone,\n",
        "}\n",
        "\n",
        "\n",
        "with custom_object_scope(custom_objects):\n",
        "    yolo.fpn_depth = 1\n",
        "    yolo = tf.keras.models.load_model(\"best_model3.keras\", compile=True)"
      ],
      "metadata": {
        "id": "b7t-AqBuE6Yn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a quick and dirty way to re-load the model between epochs."
      ],
      "metadata": {
        "id": "8XNjlHU6jMVx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First, reinitialize the YOLO model (must match the trained model config)\n",
        "yolo = keras_cv.models.YOLOV8Detector(\n",
        "    num_classes=len(class_mapping),\n",
        "    bounding_box_format=\"xyxy\",\n",
        "    backbone=keras_cv.models.YOLOV8Backbone.from_preset(\"yolo_v8_xl_backbone_coco\"),\n",
        "    fpn_depth=2,\n",
        ")\n",
        "\n",
        "# Now, load the best weights\n",
        "yolo.load_weights(\"best_model3.keras\")"
      ],
      "metadata": {
        "id": "8efa00ixAVnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ready it up for another run, this time with different than initial parameters if needed (fine-fine-tuning)."
      ],
      "metadata": {
        "id": "xfhyd8QHjRUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yolo.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001), # Play with this\n",
        "    classification_loss=\"binary_crossentropy\",\n",
        "    box_loss=\"ciou\",\n",
        ")\n",
        "\n",
        "yolo.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=10,\n",
        "    callbacks=[checkpoint_callback],\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 837
        },
        "id": "InLhlbYfb2aY",
        "outputId": "3eca97f6-8abc-4d6b-a119-fd77abfd36f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py:237: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
            "Expected: ['keras_tensor_8953']\n",
            "Received: inputs=Tensor(shape=(4, 640, 640, 3))\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12s/step - box_loss: 2.2878 - class_loss: 0.6385 - loss: 2.9263 \n",
            "Epoch 1: val_loss improved from 3.55063 to 3.41001, saving model to best_model.keras\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m835s\u001b[0m 13s/step - box_loss: 2.2879 - class_loss: 0.6394 - loss: 2.9273 - val_box_loss: 2.6931 - val_class_loss: 0.7169 - val_loss: 3.4100\n",
            "Epoch 2/10\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12s/step - box_loss: 2.2347 - class_loss: 0.6592 - loss: 2.8940 \n",
            "Epoch 2: val_loss improved from 3.41001 to 3.39160, saving model to best_model.keras\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m774s\u001b[0m 13s/step - box_loss: 2.2347 - class_loss: 0.6598 - loss: 2.8945 - val_box_loss: 2.6767 - val_class_loss: 0.7149 - val_loss: 3.3916\n",
            "Epoch 3/10\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12s/step - box_loss: 2.1991 - class_loss: 0.6356 - loss: 2.8347 \n",
            "Epoch 3: val_loss improved from 3.39160 to 3.33421, saving model to best_model.keras\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m760s\u001b[0m 13s/step - box_loss: 2.1991 - class_loss: 0.6363 - loss: 2.8354 - val_box_loss: 2.6223 - val_class_loss: 0.7119 - val_loss: 3.3342\n",
            "Epoch 4/10\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12s/step - box_loss: 2.1879 - class_loss: 0.6230 - loss: 2.8109 \n",
            "Epoch 4: val_loss improved from 3.33421 to 3.30604, saving model to best_model.keras\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m787s\u001b[0m 13s/step - box_loss: 2.1876 - class_loss: 0.6236 - loss: 2.8112 - val_box_loss: 2.6026 - val_class_loss: 0.7034 - val_loss: 3.3060\n",
            "Epoch 5/10\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12s/step - box_loss: 2.1501 - class_loss: 0.6156 - loss: 2.7657 \n",
            "Epoch 5: val_loss did not improve from 3.30604\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m774s\u001b[0m 13s/step - box_loss: 2.1500 - class_loss: 0.6163 - loss: 2.7663 - val_box_loss: 2.6047 - val_class_loss: 0.7060 - val_loss: 3.3107\n",
            "Epoch 6/10\n",
            "\u001b[1m13/60\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9:09\u001b[0m 12s/step - box_loss: 2.1464 - class_loss: 0.5991 - loss: 2.7455"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-207-5a0293d99d48>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m yolo.fit(\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             ):\n\u001b[0;32m--> 219\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1682\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1683\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1684\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1685\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a function to print the images from validation dataset, and predict the objects in each one, with the finished model. We can then view the predictions it made and see if it indeed finds the objects we trained it to find, and if the bounding boxes are of appropriate sizes and shapes."
      ],
      "metadata": {
        "id": "mTp6tQVdetgT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_entire_dataset(model, dataset, bounding_box_format, class_mapping):\n",
        "    import math\n",
        "    from keras_cv import visualization\n",
        "\n",
        "    # We'll loop over each batch in the dataset\n",
        "    batch_index = 0\n",
        "    for images, y_true in dataset:\n",
        "        # Run inference on this batch\n",
        "        y_pred = model.predict(images, verbose=0)\n",
        "        # print(y_pred)\n",
        "        # Decide how many rows and columns to display\n",
        "        # e.g., if you have 4 images in a batch, you might do 2x2\n",
        "        batch_size = images.shape[0]\n",
        "        cols = 2\n",
        "        rows = math.ceil(batch_size / cols)  # enough rows to fit the batch\n",
        "\n",
        "        visualization.plot_bounding_box_gallery(\n",
        "            images,\n",
        "            value_range=(0, 255),\n",
        "            bounding_box_format=bounding_box_format,\n",
        "            y_pred=y_pred,\n",
        "            y_true=y_true,\n",
        "            scale=4,\n",
        "            rows=rows,\n",
        "            cols=cols,\n",
        "            show=True,\n",
        "            font_scale=0.7,\n",
        "            class_mapping=class_mapping,\n",
        "        )\n",
        "        batch_index += 1\n",
        "        print(f\"Displayed batch {batch_index}\")\n"
      ],
      "metadata": {
        "id": "OK8FSNVWRIVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Change the confidence score and overlapping threshold (wether to display a detection the model is not confident about, and how many overlapping, maybe similar detection per object to show or not, respectively)."
      ],
      "metadata": {
        "id": "S1291aeYmdqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup inference parameters\n",
        "yolo.score_threshold = 0.8\n",
        "yolo.nms_iou_threshold = 0.2"
      ],
      "metadata": {
        "id": "o3WI9c1lz5Vx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calling this function (with either \"train_ds\" to predict on training data or val_ds to predict on validation data) gives a feeling of the model's detection ability."
      ],
      "metadata": {
        "id": "pBxrcOmvj1HG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run inference on validation set\n",
        "visualize_entire_dataset(\n",
        "    model=yolo,\n",
        "    dataset=val_ds, # or train_ds\n",
        "    bounding_box_format=\"xyxy\",\n",
        "    class_mapping=class_mapping\n",
        ")"
      ],
      "metadata": {
        "id": "hYBGhBbLRFAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final result and current capabilities of the model:"
      ],
      "metadata": {
        "id": "Rxo0BEt91aV_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Description](result_of_model1.png)\n"
      ],
      "metadata": {
        "id": "M1nDESLS1gQ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusions and remarks ###\n",
        "This concludes this project. Here is a quick summary:\n",
        "\n",
        "1. Created a dataset manually, labeling a few hard to detect objects in various environmental conditions (lighting, angles, obstructions in view, sizes, etc)\n",
        "\n",
        "2. Planned and executed a deep learning architecture, namely a CNN pre-trained model, fine-tuned on our dataset, to perform object detection.\n",
        "\n",
        "3. Invested time into trying to better the results, and understand the issues along the way that must be taken care of:\n",
        "\n",
        "  3.1 For example, at first I've tried to train the model on a much larger dataset, consisting of 13 different classes, making very poor results, probably because I made very different object share classes, and the preparation of the data was not good enough (bounding boxes were not tight enough, etc).\n",
        "\n",
        "  3.2 It also took a long time, to decide how long and which model to use for training.\n",
        "\n",
        "  3.3 Also, hyper-parameters played a crucial role in the results - for example, switching from binary_crossentropy to focal_loss had a major imporvement on results!\n",
        "\n",
        "4. Print our result of the object detection task on new data the model did not train on.\n",
        "\n",
        "5. For future: possible to build a much larger dataset, creating a different class per object in images. Find many different angles, sizes, lighting conditions anr more for each. Train a large and popular model, suitable for this task for a long time on the data, and incorporate many augmentation techniquies. Then, we can expect good results, and a model that can detect object in video games in real-time (or semi real-time, probably about 15 fps is achievable with non-industrial technology and resources).\n",
        "\n",
        "6. On a personal note, i've had the change to wrestle with machinary of object detection, and understand what it takes to train a model to perform such a task. I've also learned alot about the existing software packages that help perform this mission.\n",
        "I've learned alot about the technology of CNN models itself, understanding every part of the architecture, not only theoretiacly but by actually having to need it to run successfully, integerated with all the rest of the system. And I've also learned alot about how to use many online tools like Google Collab, Gemini, Robowflow, ChatGPT, and read a few articles on the matter, to quickly find a working architechture suitable for what I've decided to do (object detection for Baldure's Gate 3).\n",
        "\n",
        "7. This is a very hand-on course. Currently I work full-time at Intel inc. and also actively serve in the veteran IDF army. I find a balance between work, study and military service, since the 2023 October war started. I did not fully use the potential of this course, by incorporating extensive EDA techniques and scoring performance programmatically mainly due to time and effort constraints unfortunately. Hopefully when I'm ready for the masters degree at The Open University of Israel, I'll have more time on my hands to really invest and implement myself in data science which is a very important, relevant and practical subject in computer science these days. This is the last course I'm doing in the BA. CS studies, and even just passing this course, will conclude my degree with an average of 83.\n",
        "I want to thank Dr. Idan Alter for his mentorship and comprehensive feedback on this project and data science in general, and wish him all the best in his efforts.\n",
        "\n",
        "Respectfully, best regards, Or."
      ],
      "metadata": {
        "id": "VNlb3AqdfDQv"
      }
    }
  ]
}